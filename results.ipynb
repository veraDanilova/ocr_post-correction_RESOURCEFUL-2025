{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-20T11:53:27.053426Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Re-used notebook from:\n",
    "https://github.com/Shef-AIRE/llms_post-ocr_correction/blob/main/results.ipynb\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "from IPython.core.getipython import get_ipython\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig, pipeline\n",
    "import Levenshtein\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e09f5876318a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute character error rate (CER)\n",
    "def cer(prediction, target):\n",
    "    distance = Levenshtein.distance(prediction, target)\n",
    "    return distance / len(target)\n",
    "\n",
    "# Update the results dataframe with CER reduction values\n",
    "def get_results(data, preds):\n",
    "    results = data.to_pandas()\n",
    "    results['Model Correction'] = preds\n",
    "    # results = results.rename(columns={'CER': 'old_CER'})\n",
    "    results['old_CER'] = results.apply(lambda row: cer(row['OCR Text'], row['Ground Truth']), axis=1)\n",
    "    results['new_CER'] = results.apply(lambda row: cer(row['Model Correction'], row['Ground Truth']), axis=1)\n",
    "    results['CER_reduction'] = ((results['old_CER'] - results['new_CER']) / (results['old_CER'])) * 100\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da40395e623f3896",
   "metadata": {},
   "source": [
    "## BART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd900d7-f315-457e-b638-c2a3d8e6e2de",
   "metadata": {},
   "source": [
    "Generate post-OCR corrections with BART and save to `results` folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "978356b1fb7b3c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [02:41<00:00,  2.76it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'model/***'\n",
    "\n",
    "test = pd.read_csv('***.csv')\n",
    "test = Dataset.from_pandas(test)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "generator = pipeline('text2text-generation', model=model.to('cuda'), tokenizer=tokenizer, device='cuda', max_length=1024)\n",
    "\n",
    "preds = []\n",
    "for sample in tqdm(test):\n",
    "    preds.append(generator(sample['OCR Text'])[0]['generated_text'])\n",
    "\n",
    "results = get_results(test, preds)\n",
    "results.to_csv('results/***.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7792388872aa38d1",
   "metadata": {},
   "source": [
    "## Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21986848-7fa1-4016-8c1c-4b727b349536",
   "metadata": {},
   "source": [
    "Generate post-OCR corrections with Llama 2 and save to `results` folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a1c28f69dd33897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c18199e126e4433a9755fd506ea1bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/376 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "100%|██████████| 376/376 [29:42<00:00,  4.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model and configure the callback\n",
    "model_id = '***'\n",
    "model_dir = f'***/model/{model_id}'\n",
    "\n",
    "test = pd.read_csv('***.csv')\n",
    "test = Dataset.from_pandas(test)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "i = 0\n",
    "preds = []\n",
    "\n",
    "cell = '''\n",
    "prompt = f\"\"\"### Anweisung:\n",
    "Korrigieren Sie die OCR-Fehler im bereitgestellten Text.\n",
    "\n",
    "### Eingabe:\n",
    "{test[i]['OCR Text']}\n",
    "\n",
    "### Antwort:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, max_length=1024, return_tensors='pt', truncation=True).input_ids.cuda()\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(input_ids=input_ids, max_new_tokens=1024, do_sample=True, temperature=0.7, top_p=0.1, top_k=40)\n",
    "pred = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):].strip()\n",
    "preds.append(pred)\n",
    "i += 1\n",
    "'''\n",
    "\n",
    "ipython = get_ipython()\n",
    "for _ in tqdm(range(len(test))):\n",
    "    ipython.run_cell(cell)\n",
    "\n",
    "results = get_results(test, preds)\n",
    "results.to_csv('results/***.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
